<div align="center">

# Fashion MNIST Classification

### _Deep CNN & Traditional ML â€” A Comparative Study_

**RAS598 Â· Fall 2024 Â· Arizona State University**

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

`Karan Athrey` Â· `Abhijit Sinha` Â· `Anusha Chatterjee`

[![Python](https://img.shields.io/badge/-Python-05122A?style=flat&logo=python)](#)&nbsp;
[![TensorFlow](https://img.shields.io/badge/-TensorFlow-05122A?style=flat&logo=tensorflow)](#)&nbsp;
[![scikit-learn](https://img.shields.io/badge/-Sklearn-05122A?style=flat&logo=scikitlearn)](#)&nbsp;
[![NumPy](https://img.shields.io/badge/-NumPy-05122A?style=flat&logo=numpy)](#)&nbsp;
[![Plotly](https://img.shields.io/badge/-Plotly-05122A?style=flat&logo=plotly)](#)

</div>

<br>

> **TL;DR** â€” We built 7 models to classify 70k fashion images into 10 categories. Our ResNet and CNN both hit **91% accuracy**, outperforming traditional ML baselines (83â€“89%). Grad-CAM confirms the models focus on the right visual features.

<br>

## â—ˆ What This Project Does

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚          Fashion MNIST (70,000 images)       â”‚
                    â”‚          28Ã—28 grayscale Â· 10 classes        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â–¼                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Deep Learning  â”‚         â”‚  Traditional ML  â”‚
                â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚         â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                â”‚  â€¢ ResNet       â”‚         â”‚  â€¢ Logistic Reg  â”‚
                â”‚  â€¢ CNN          â”‚         â”‚  â€¢ SVM           â”‚
                â”‚                 â”‚         â”‚  â€¢ Random Forest â”‚
                â”‚                 â”‚         â”‚  â€¢ KNN           â”‚
                â”‚                 â”‚         â”‚  â€¢ MLP           â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                           â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Compare & Analyze  â”‚
                            â”‚  Accuracy Â· F1 Â· CM â”‚
                            â”‚  Grad-CAM Â· Plotly  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<br>

## â—ˆ The Dataset

Fashion MNIST serves as a modern replacement for the classic MNIST digits â€” same format, harder problem.

```
60,000 training images  Â·  10,000 test images  Â·  6,000 per class (balanced)
```

<table>
<tr>
<td width="50%">

| ID | Category | ID | Category |
|:--:|----------|:--:|----------|
| 0 | T-shirt/top | 5 | Sandal |
| 1 | Trouser | 6 | Shirt |
| 2 | Pullover | 7 | Sneaker |
| 3 | Dress | 8 | Bag |
| 4 | Coat | 9 | Ankle boot |

</td>
<td width="50%">

**Pixel Statistics**
```
Mean intensity   â– â– â– â– â– â– â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  72.94
Median intensity â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0.00
Std deviation    â– â– â– â– â– â– â– â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  90.02
```
Most pixels are black (background) â€” normalization is essential.

</td>
</tr>
</table>

<br>

## â—ˆ Preprocessing Pipeline

```
 â‘  LOAD           â‘¡ NORMALIZE        â‘¢ RESHAPE           â‘£ SPLIT
 â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Load from        Pixel values       (28,28,1) for CNN   Train / Val
 Keras API        Ã· 255 â†’ [0,1]     Flatten for ML      / Test sets
```

For traditional ML models, images are flattened from 28Ã—28 matrices into 784-dimensional feature vectors.

<br>

## â—ˆ Model Architectures

<details>
<summary><b>ğŸ”· ResNet â€” Residual Network</b> &nbsp;(click to expand)</summary>
<br>

```
Input(28Ã—28Ã—1)
  â”‚
  â”œâ”€â–º Conv2D(32, 3Ã—3, ReLU)
  â”œâ”€â–º MaxPooling2D(2Ã—2)
  â”‚
  â”œâ”€â–º â•”â•â• ResBlock â•â•â•—
  â”‚   â•‘ Conv2D(32)   â•‘
  â”‚   â•‘ BatchNorm    â•‘
  â”‚   â•‘ Conv2D(32)   â•‘â”€â”€â–º(+)â”€â”€â–º ReLU
  â”‚   â•‘ BatchNorm    â•‘   â–²
  â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
  â”‚          skip â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º MaxPooling2D(2Ã—2)
  â”‚
  â”œâ”€â–º â•”â•â• ResBlock â•â•â•—
  â”‚   â•‘ (same as     â•‘
  â”‚   â•‘  above)      â•‘
  â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â”‚
  â”œâ”€â–º GlobalAveragePooling2D
  â”œâ”€â–º Flatten
  â””â”€â–º Dense(10, Softmax)

Total params: 38,154 (148 KB)
```

**Why ResNet?** Skip connections solve vanishing gradients, enabling deeper training without degradation.

</details>

<details>
<summary><b>ğŸ”¶ CNN â€” Convolutional Neural Network</b> &nbsp;(click to expand)</summary>
<br>

```
Input(28Ã—28Ã—1)
  â”‚
  â”œâ”€â–º Conv2D(32, 3Ã—3)  â†’ 26Ã—26Ã—32
  â”œâ”€â–º MaxPool(2Ã—2)     â†’ 13Ã—13Ã—32
  â”‚
  â”œâ”€â–º Conv2D(64, 3Ã—3)  â†’ 11Ã—11Ã—64
  â”œâ”€â–º MaxPool(2Ã—2)     â†’  5Ã—5Ã—64
  â”‚
  â”œâ”€â–º Conv2D(128, 3Ã—3) â†’  3Ã—3Ã—128
  â”‚
  â”œâ”€â–º Flatten           â†’ 1,152
  â”œâ”€â–º Dense(128, ReLU)
  â””â”€â–º Dense(10, Softmax)
```

**Design logic:** Filters increase (32 â†’ 64 â†’ 128) while spatial dims decrease â€” progressive abstraction from edges to complex patterns.

</details>

<details>
<summary><b>â¬œ Traditional ML Models</b> &nbsp;(click to expand)</summary>
<br>

All traditional models operate on **flattened 784-d vectors** (no spatial structure).

| Model | Approach | Tradeoff |
|-------|----------|----------|
| **Logistic Regression** | Linear decision boundary | Fast training, weak on complex patterns |
| **SVM** | Kernel-based hyperplane (RBF) | Strong accuracy, expensive at scale |
| **Random Forest** | Ensemble of decision trees | Good balance of speed and accuracy |
| **KNN** | Distance-based classification | Memory-heavy, slow inference |
| **MLP** | Fully connected hidden layers | Non-linear but no spatial awareness |

</details>

<br>

## â—ˆ Results at a Glance

```
Accuracy (%)
â”‚
â”‚  91    91                        89               
â”‚  â”Œâ”€â”€â” â”Œâ”€â”€â”                87    â”Œâ”€â”€â”           85  
â”‚  â”‚â–“â–“â”‚ â”‚â–“â–“â”‚       83     â”Œâ”€â”€â”    â”‚â–‘â–‘â”‚   â”€â”€â”€    â”Œâ”€â”€â”
â”‚  â”‚â–“â–“â”‚ â”‚â–“â–“â”‚      â”Œâ”€â”€â”    â”‚â–‘â–‘â”‚    â”‚â–‘â–‘â”‚   KNN    â”‚â–‘â–‘â”‚
â”‚  â”‚â–“â–“â”‚ â”‚â–“â–“â”‚      â”‚â–‘â–‘â”‚    â”‚â–‘â–‘â”‚    â”‚â–‘â–‘â”‚          â”‚â–‘â–‘â”‚
â”‚  â”‚â–“â–“â”‚ â”‚â–“â–“â”‚      â”‚â–‘â–‘â”‚    â”‚â–‘â–‘â”‚    â”‚â–‘â–‘â”‚          â”‚â–‘â–‘â”‚
â””â”€â”€â”´â”€â”€â”´â”€â”´â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€
  ResNet  CNN    LogReg    SVM      RF           MLP

  â–“â–“ Deep Learning    â–‘â–‘ Traditional ML
```

<table>
<tr>
<td>

**Training Dynamics**

| | ResNet | CNN |
|---|:---:|:---:|
| Train Acc | 95% | 96% |
| Val Acc | 91% | 91% |
| Train Loss | 0.10 | 0.12 |
| Val Loss | 0.30 | 0.28 |
| Epochs | 20 | 10 |

</td>
<td>

**Key Takeaways**

- Both deep models converge to **91% validation accuracy**
- ~4â€“5% train-val gap â†’ mild overfitting, acceptable generalization
- ResNet needs **2Ã— more epochs** but uses fewer parameters
- CNN trains faster with slightly higher training accuracy
- Traditional ML tops out at **~89%** (Random Forest)

</td>
</tr>
</table>

<br>

## â—ˆ Per-Class Performance

```
                 Precision    Recall    F1-Score
                 â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€
 T-shirt/top        0.81      0.91       0.86   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–‘
 Trouser             0.98      0.99       0.98   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ª  â˜…
 Pullover            0.87      0.88       0.88   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–‘
 Dress               0.95      0.88       0.91   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–‘
 Coat                0.88      0.87       0.88   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–‘
 Sandal              0.96      0.99       0.97   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ª  â˜…
 Shirt               0.79      0.71       0.74   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–‘â–‘â–‘  âš 
 Sneaker             0.94      0.98       0.96   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ª
 Bag                 0.97      0.99       0.98   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ª  â˜…
 Ankle boot          1.00      0.92       0.96   â–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ªâ–ª
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Overall Accuracy                          0.91
```

â˜… Top performers &nbsp;&nbsp; âš  Needs improvement

<br>

## â—ˆ Confusion Matrix

<p align="center">
  <img src="confusion_matrix.png" alt="Confusion Matrix" width="650"/>
</p>

**Where the model gets confused:**

```
 Shirt â”€â”€â”€â”€â”€â”€â”€â”€ 156 cases â”€â”€â”€â”€â–º T-shirt/top     (similar silhouettes)
 Ankle boot â”€â”€â”€â”€ 56 cases â”€â”€â”€â”€â–º Sneaker          (both are footwear)
 Coat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 40 cases â”€â”€â”€â”€â–º Pullover         (overlapping shapes)
 Pullover â”€â”€â”€â”€â”€â”€ 43 cases â”€â”€â”€â”€â–º Coat             (mutual confusion)
```

The hardest distinction is **Shirt vs. T-shirt/top** â€” at 28Ã—28 resolution, collar and sleeve differences are barely perceptible.

<br>

## â—ˆ Grad-CAM Interpretability

<p align="center">
  <img src="grad_cam.png" alt="Grad-CAM Visualization" width="650"/>
</p>

<table>
<tr>
<td width="33%" align="center"><b>Original Image</b><br><sub>True label: Ankle boot</sub></td>
<td width="33%" align="center"><b>Grad-CAM Heatmap</b><br><sub>Yellow/red = high attention</sub></td>
<td width="33%" align="center"><b>Superimposed</b><br><sub>Predicted: Ankle boot âœ“</sub></td>
</tr>
</table>

The model correctly focuses on the **boot shaft and sole** â€” confirming it uses semantically meaningful visual cues, not background noise or dataset artifacts.

<br>

## â—ˆ Getting Started

```bash
# clone
git clone https://github.com/AByteOfAI/fashion_mnist.git
cd fashion_mnist

# install dependencies
pip install tensorflow scikit-learn numpy matplotlib seaborn plotly
```

**Minimal example:**

```python
from tensorflow.keras.datasets import fashion_mnist

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0          # normalize
X_train_cnn = X_train.reshape(-1, 28, 28, 1)                # reshape for CNN
```

Full source code is available in `Report_Part1.pdf` and `Report_Part2.pdf`.

<br>

## â—ˆ Repository Contents

```
.
â”œâ”€â”€ README.md               â† you are here
â”œâ”€â”€ RASFINALppt.pdf          â† presentation slides (21 slides)
â”œâ”€â”€ Report_Part1.pdf         â† full report: methodology + results + code
â”œâ”€â”€ Report_Part2.pdf         â† report continued: source code + outputs
â”œâ”€â”€ confusion_matrix.png     â† ResNet confusion matrix (10Ã—10)
â””â”€â”€ grad_cam.png             â† Grad-CAM heatmap for ankle boot
```

<br>

## â—ˆ Limitations & Future Directions

| Current Limitation | Proposed Solution |
|----|-----|
| Grayscale-only, white background images | Train on colored datasets like DeepFashion |
| Shirt / T-shirt confusion (F1: 0.74) | Attention mechanisms for collar/sleeve regions |
| Mild overfitting (~4â€“5% gap) | Dropout, stronger augmentation |
| No transfer learning explored | Fine-tune MobileNet / EfficientNet |
| Single-dataset evaluation | Cross-dataset generalization testing |

<br>

## â—ˆ Authors

<table>
<tr>
<td align="center"><b>Karan Athrey</b><br><a href="mailto:kathrey@asu.edu">kathrey@asu.edu</a></td>
<td align="center"><b>Abhijit Sinha</b><br><a href="mailto:asinh117@asu.edu">asinh117@asu.edu</a></td>
<td align="center"><b>Anusha Chatterjee</b><br><a href="mailto:achatt53@asu.edu">achatt53@asu.edu</a></td>
</tr>
</table>

---

<div align="center">
<sub>Arizona State University Â· RAS598: Robotic and Autonomous Systems Â· Fall 2024</sub>
<br><br>
<sub>If this was helpful, a â­ on the repo would be appreciated!</sub>
</div>
